---
kind: Job
apiVersion: batch/v1
metadata:
{% if uperf.serviceip is sameas true %}
  name: 'uperf-client-{{item.spec.clusterIP}}-{{ trunc_uuid }}'
{% else %}
  name: 'uperf-client-{{item.status.podIP}}-{{ trunc_uuid }}'
{% endif %}
  namespace: '{{ operator_namespace }}'
spec:
  template:
    metadata:
      labels:
        app: uperf-bench-client-{{ trunc_uuid }}
{% if uperf.multus.enabled is sameas true %}
      annotations:
        k8s.v1.cni.cncf.io/networks: {{ uperf.multus.client }}
{% endif %}
    spec:
{% if uperf.hostnetwork is sameas true %}
      hostNetwork : true
      serviceAccountName: benchmark-operator
      serviceAccount: benchmark-operator
{% endif %}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: type
                  operator: In
                  values:
                  - uperf-bench-server-{{ trunc_uuid }}
              topologyKey: kubernetes.io/hostname
      containers:
      - name: benchmark
        image: "quay.io/cloud-bulldozer/uperf:latest"
        imagePullPolicy: Always
        wait: true
        command: ["/bin/sh", "-c"]
        args:
{% if uperf.serviceip is sameas true %}
          - "export serviceip=true;
            export h={{item.spec.clusterIP}};
{% else %}
{% if uperf.multus.client is defined %}
          - "export h={{ (item['metadata']['annotations']['k8s.v1.cni.cncf.io/networks-status'] | from_json)[1]['ips'][0] }};
{% else %}
          - "export h={{item.status.podIP}};
{% endif %}
{% endif %}
{% if elasticsearch is defined %}
{% if elasticsearch.server is defined %}
             export es={{elasticsearch.server}};
             export es_port={{elasticsearch.port}};
{% endif %}
{% endif %}
{% if test_user is defined %}
             export test_user={{test_user}};
{% endif %}
{% if uperf.pin is sameas true %}
             export client_node={{uperf.pin_client}};
             export server_node={{uperf.pin_server}};
{% else %}
             export client_node=UNSET;
             export server_node=UNSET;
{% endif %}
{% if redis_collection is defined %}
             export redis_collection={{redis_collection}};
{% else %}
             export redis_collection=False;
{% endif %}
             export clustername={{clustername}};
             export hostnet={{uperf.hostnetwork}};
             export ips=$(hostname -I);
             export uuid={{uuid}};
             export redis_port={{bo.resources[0].spec.containers[2].ports[0].containerPort}};
             export bo_ip={{bo.resources[0].status.podIP}};
             while true; do
               if [[ $(redis-cli -h {{bo.resources[0].status.podIP}} get start) =~ 'true' ]]; then
{% for test in uperf.test_types %}
{% for proto in uperf.protos %}
{% for size in uperf.sizes %}
{% for nthr in uperf.nthrs %}
                 cat /tmp/uperf-test/uperf-{{test}}-{{proto}}-{{size}}-{{nthr}};
                 for i in `seq 1 {{uperf.samples}}`; do
                   python /opt/snafu/uperf-wrapper/uperf-wrapper.py -w /tmp/uperf-test/uperf-{{test}}-{{proto}}-{{size}}-{{nthr}} -r $i --resourcetype {{resource_kind}};
                 done;
{% endfor %}
{% endfor %}
{% endfor %}
{% endfor %}
               else
                 continue;
               fi;
               break;
             done;
             redis-cli -h {{bo.resources[0].status.podIP}} set start false"
        volumeMounts:
          - name: config-volume
            mountPath: "/tmp/uperf-test"
      volumes:
        - name: config-volume
          configMap:
            name: uperf-test-{{ trunc_uuid }}
      restartPolicy: OnFailure
{% if uperf.pin is sameas true %}
      nodeSelector:
          kubernetes.io/hostname: '{{ uperf.pin_client }}'
{% endif %}
