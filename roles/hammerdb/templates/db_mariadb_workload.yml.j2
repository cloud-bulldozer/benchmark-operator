---
apiVersion: batch/v1
kind: "job"
metadata:
  name: "{{ ansible_operator_meta.name }}-workload-{{ trunc_uuid }}"
  namespace: "{{ operator_namespace }}"
spec:
  ttlSecondsAfterFinished: 600
  template:
    metadata:
      labels:
        app: hammerdb_workload-{{ trunc_uuid }}
        type: {{ ansible_operator_meta.name }}-bench-workload-{{ trunc_uuid }}
        benchmark-operator-uuid: {{ uuid }}
    spec:
{% if workload_args.pin is sameas true %}
      nodeSelector:
        kubernetes.io/hostname: '{{ workload_args.pin_node }}'
{% endif %}
{% if workload_args.runtime_class is defined %}
      runtimeClassName: "{{ workload_args.runtime_class }}"
{% endif %}
      containers:
      - name: hammerdb
{% if workload_args.resources is sameas true %}
        resources:
          requests:
            cpu: {{ workload_args.requests_cpu | default("200m") }}
            memory: {{ workload_args.requests_memory | default("100Mi") }}
          limits:
            cpu: {{ workload_args.limits_cpu }}
            memory: {{ workload_args.limits_memory }}
{% endif %}
        image: {{ workload_args.image | default('quay.io/cloud-bulldozer/hammerdb:master') }}
        imagePullPolicy: Always
        env:
          - name: uuid
            value: "{{ uuid }}"
          - name: test_user
            value: "{{ test_user | default("ripsaw") }}"
          - name: clustername
            value: "{{ clustername }}"
{% if elasticsearch is defined %}
          - name: es
            value: "{{ elasticsearch.url }}"
          - name: es_index
            value: "{{ elasticsearch.index_name | default("ripsaw-hammerdb") }}"
          - name: parallel
            value: "{{ elasticsearch.parallel | default(false) }}"
{% endif %}
{% if prometheus is defined %}
          - name: prom_es
            value: "{{ prometheus.es_url }}"
          - name: prom_parallel
            value: "{{ prometheus.es_parallel | default(false) }}"
          - name: prom_token
            value: "{{ prometheus.prom_token | default() }}"
          - name: prom_url
            value: "{{ prometheus.prom_url | default() }}"
{% endif %}
        command: ["/bin/sh", "-c"]
        args:
          - "/usr/local/bin/uid_entrypoint;
             export db_type={{workload_args.db_type}};
             export timed_test={{workload_args.timed_test}};
             export db_server={{workload_args.db_server}};
             export db_port={{workload_args.db_port}};
             export db_warehouses={{workload_args.db_warehouses}};
             export db_num_workers={{workload_args.db_num_workers}};
             export db_user={{workload_args.db_user}};
             export db_pass={{workload_args.db_pass}};
             export db_name={{workload_args.db_name}};
             export transactions={{workload_args.transactions}};
             export raiseerror={{workload_args.raiseerror}};
             export keyandthink={{workload_args.keyandthink}};
             export driver={{workload_args.driver}};
             export rampup={{workload_args.rampup}};
             export runtime={{workload_args.runtime}};
             export allwarehouse={{workload_args.allwarehouse}};
             export timeprofile={{workload_args.timeprofile}};
             export async_scale={{workload_args.async_scale}};
             export async_client={{workload_args.async_client}};
             export async_verbose={{workload_args.async_verbose}};
             export async_delay={{workload_args.async_delay}};
             export samples={{workload_args.samples}};
             export db_mysql_storage_engine={{workload_args.db_mysql_storage_engine}};
             export db_mysql_partition={{workload_args.db_mysql_partition}};
{% if workload_args.es_custom_field is sameas true %}
             export es_ocp_version={{workload_args.es_ocp_version}};
             export es_cnv_version={{workload_args.es_cnv_version}};
             export es_db_version={{workload_args.es_db_version}};
             export es_os_version={{workload_args.es_os_version}};
             export es_kind={{workload_args.es_kind}};
{% endif %}
             cd /hammer;
             run_snafu --tool hammerdb -u {{ uuid }}"
        volumeMounts:
        - name: hammerdb-workload-volume
          mountPath: "/workload"
          readOnly: false
      volumes:
      - name: hammerdb-workload-volume
        configMap:
          name: "{{ ansible_operator_meta.name }}-workload-{{ trunc_uuid }}"
          defaultMode: 0640
      restartPolicy: OnFailure
{% include "metadata.yml.j2" %}

