---

- name: Get current state
  k8s_facts:
    api_version: ripsaw.cloudbulldozer.io/v1alpha1
    kind: Benchmark
    name: '{{ ansible_operator_meta.name }}'
    namespace: '{{ operator_namespace }}'
  register: resource_state

- operator_sdk.util.k8s_status:
    api_version: ripsaw.cloudbulldozer.io/v1alpha1
    kind: Benchmark
    name: "{{ ansible_operator_meta.name }}"
    namespace: "{{ operator_namespace }}"
    status:
      state: Building
      complete: false
  when: resource_state.resources[0].status.state is not defined

- name: Get current state - If it has changed
  k8s_facts:
    api_version: ripsaw.cloudbulldozer.io/v1alpha1
    kind: Benchmark
    name: '{{ ansible_operator_meta.name }}'
    namespace: '{{ operator_namespace }}'
  register: resource_state

- name: Capture operator information
  k8s_facts:
    kind: Pod
    api_version: v1
    namespace: '{{ operator_namespace }}'
    label_selectors:
      - name = benchmark-operator
  register: bo
  
- block:
    #
    # This block is for scale mode where client and server pods are spreaded
    # across all eligible nodes
    #
  - name: List Nodes Labeled as Workers
    k8s_info:
      api_version: v1
      kind: Node
      label_selectors:
        - "node-role.kubernetes.io/worker="
    register: node_list
    no_log: True
  
  - name: Isolate Worker Role Hostnames
    set_fact:
      worker_node_list: "{{ node_list | json_query('resources[].metadata.labels.\"kubernetes.io/hostname\"') | list }}"
 
  - name: List Nodes Labeled with {{ workload_args.exclude_label }}
    k8s_info:
      api_version: v1
      kind: Node
      label_selectors:
        - '{{ item }}'
    with_items: "{{ workload_args.exclude_labels }}"
    register: exclude_node_list
    when: workload_args.exclude_labels is defined and workload_args.exclude_labels | length > 0
  
  - name: Isolate Worker Role Hostnames for label  {{ workload_args.exclude_label }}
    set_fact:
      worker_node_exclude_list: "{{ exclude_node_list | json_query('results[].resources[].metadata.name') }}"
  
  - name: Exclude labeled nodes
    set_fact:
        worker_node_list: "{{ worker_node_list | difference(worker_node_exclude_list) }}"
    when: workload_args.exclude_labels is defined and workload_args.exclude_labels | length > 0
    #
    # Compute node and pod limits using CR params while taking into account 
    # of the actual number of nodes available in the system
    #
  - name: init pod and node low/hi idx
    set_fact:
        pod_low_idx: "{{ workload_args.density_range[0] | default('1')|int - 1 }}"
        pod_hi_idx: "{{ workload_args.density_range[1] | default('1')|int - 1 }}"
        node_low_idx: "{{ workload_args.node_range[0] | default('1')|int - 1 }}"
        node_hi_idx: "{{ workload_args.node_range[1]  | default('1')|int - 1 }}"
    #
    # Next sanity check and massage the indices if necessary.
    # We shall complete gracefully and not iterate wildly.
    #
  - name: Adjust node_hi_idx if cluster has less nodes
    set_fact:
        node_hi_idx: "{{ worker_node_list|length| default('0')|int -1 }}"
    when: "node_hi_idx|int >= worker_node_list|length| default('0')|int "

  - name: Adjust node_low_idx if necessary 
    set_fact:
        node_low_idx: "{{node_hi_idx|int}}"
    when: "node_low_idx|int > node_hi_idx|int"

  - name: Adjust pod_low_idx if necessary
    set_fact:
        pod_low_idx: "{{pod_hi_idx|int}}"
    when: "pod_low_idx|int > pod_hi_idx|int"

  - name: Record num server pods using new worker_node_list
    # in Scale mode, num server pods = num_node * number_pod
    set_fact:
      num_server_pods: "{{ (node_hi_idx|int+1) * (pod_hi_idx|int+1) }}"
 
    #
    # End scale mode
    #
  when: workload_args.pin | default(False) == False

- block:
    #
    # This block is for the "pin" mode where the server and the client node
    # are specified by pin_server and pin_client variables.
    
  - name: Add "Pin" server and client node to worker list. 
    # The add order is significant as we will enumerate the server pods on
    # the first node in the list, and client pods on the second node.
    set_fact: 
        worker_node_list: "{{worker_node_list + [item]}}"
    with_items:
      - '{{workload_args.pin_server}}'
      - '{{workload_args.pin_client}}'
  #
  # In 'Pin' mode, 'pair' specifies number of pairs (classic behavior), If 'pair' 
  # is undefined use 'density_range' (new bahavior with "Scale" enhancement)
  #
  - name: Init "Pin" mode indices using 'pair'
    set_fact:
        pod_low_idx: "{{ workload_args.pair | default('1')|int - 1 }}"
        pod_hi_idx: "{{ workload_args.pair | default('1')|int - 1 }}"
        # node indices are used as client pod 'start' parameter.
        node_low_idx: "0"
        node_hi_idx: "0"
    when: workload_args.pair is defined

  - name: Init "Pin" mode indices using 'density_range'
    set_fact:
        pod_low_idx: "{{ workload_args.density_range[0] | default('1')|int - 1 }}"
        pod_hi_idx: "{{ workload_args.density_range[1] | default('1')|int - 1 }}"
        # node indices are used as client pod 'start' parameter.
        node_low_idx: "0"
        node_hi_idx: "0"
    when: workload_args.pair is not defined

  - name: Record num Pin server pods using new worker_node_list
    set_fact:
      # in Pin mode, num server pods = number of pods 
      num_server_pods: "{{ pod_hi_idx|int +1 }}"

    #
    # End pin mode where pin_client and pin_server are specified
    #
  when: workload_args.pin | default(False) == True
  
- name: Capture ServiceIP
  k8s_facts:
    kind: Service
    api_version: v1
    namespace: '{{ operator_namespace }}'
    label_selectors:
      - type = {{ ansible_operator_meta.name }}-bench-server-{{ trunc_uuid }}
  register: serviceip
  when: workload_args.serviceip is defined and workload_args.serviceip

