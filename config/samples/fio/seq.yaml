apiVersion: ripsaw.cloudbulldozer.io/v1alpha1
kind: Benchmark
metadata:
  name: berry-test-benchmark-sno-seq
  namespace: benchmark-operator
spec:
  # where elastic search is running
  elasticsearch:
    url: https://search-perfscale-dev-chmf5l4sh66lvxbnadi4bznl3a.us-west-2.es.amazonaws.com
    #url: https://shberry:ocselasticuser-2022@perf-results-elastic.apps.observability.perfscale.devcluster.openshift.com:443
    verify_cert: false
    parallel: false
  prometheus:
    #es_url: https://shberry:ocselasticuser-2022@perf-results-elastic.apps.observability.perfscale.devcluster.openshift.com:443
    es_url: https://search-perfscale-dev-chmf5l4sh66lvxbnadi4bznl3a.us-west-2.es.amazonaws.com
    prom_url: https://prometheus-k8s.openshift-monitoring.svc.cluster.local:9091
    #prom_token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImY1T0YtQ0pocE13X2tFSDBoMTZ6VE5aUDIwYTZWc3c0NTZtQ1o2N0NMTWsifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJvcGVuc2hpZnQtbW9uaXRvcmluZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwcm9tZXRoZXVzLWs4cy10b2tlbi1tdDZyOCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJwcm9tZXRoZXVzLWs4cyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjQ2ZTAwYjkzLWNlYjUtNDA5Ny04YmEzLTA5YzliNjI0MjlmMyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpvcGVuc2hpZnQtbW9uaXRvcmluZzpwcm9tZXRoZXVzLWs4cyJ9.kJTbagRKFcLbjp9aV7HrFtZyGCrbA9m3gFy6eTsBEG5tc_XkWoKjMqNM6LNFVMUxpVR9FVR8-jB1cbjv9DWlHb0UhBxfqreFfe68PmAo-2Gdwky49jH0Drv4I5X3C8UURFZbNfxO-tt-FW5joF7-4DMvliG1FUIRuZyrnXGYvBuOivYuR7trmg_x30ouzAMF5simhUUy3VA8x_TWx1t2MJblzqJnpX1aX4xZ9pIcQykBtqzeQDDdaG7ZIeUNBYlJhgy0D3ps64ADwb4e0pJxIELrCyY91H_rUaHAwW0T4Bcrt6cwt6CVhh8a3_Z6kFjbsmMjVYAs0XoV3ohIrmwJrQ
    es_parallel: true    
  clustername: seq-sno-test
  test_user: shberry
  workload:
    name: "fio_distributed"
    args:
      # if true, do large sequential write to preallocate volume before using
      prefill: true
      # for compressed volume uncomment the next line and make the cmp_bs same as bs
      # prefill_bs: 8KiB
      # number of times each test
      samples: 1
      # number of fio pods generating workload
      servers: 1
      job_timeout: 30000  
      # put all fio pods on this server
      pin_server: ''
      # test types, see fio documentation
      jobs:
        - write
        - read
          #- randwrite
          #- randread
      # I/O request sizes (also called block size)
      bs:
        - 4096KiB
        - 1024KiB
        - 256KiB
          #- 64KiB
          #- 16KiB  
          #- 8KiB
      # how many fio processes per pod
      numjobs:
        - 32
      # with libaio ioengine, number of in-flight requests per process
      iodepth: 8
      # how long to run read tests, this is TOO SHORT DURATION
      read_runtime: 300
      # how long to run write tests, this is TOO SHORT DURATION
      write_runtime: 300
      # don't start measuring until this many seconds pass, for reads
      read_ramp_time: 10
      # don't start measuring until this many seconds pass, for writes
      write_ramp_time: 10
      # size of file to access
      filesize: 5GiB
      # interval between i/o stat samples in milliseconds
      log_sample_rate: 30000
      pvcvolumemode: Block
      storageclass: odf-lvm-sbvglvm
        #storageclass: ocs-storagecluster-cephfs-new
      #storageclass: gp2  
      storagesize: 200Gi
        #drop_cache_kernel: true
        #drop_cache_rook_ceph: true
#######################################
#  EXPERT AREA - MODIFY WITH CAUTION  #
#######################################
#  global_overrides:
#  NOTE: Dropping caches as per this example can only be done if the
#        fio server is running in a privileged pod
#    - exec_prerun=bash -c 'sync && echo 3 > /proc/sys/vm/drop_caches'
  job_params:
    - jobname_match: write
      params:
        - time_based=1
        - fsync_on_close=1
        - create_on_open=1
        - runtime={{ workload_args.write_runtime }}
        - ramp_time={{ workload_args.write_ramp_time }}
    - jobname_match: read
      params:
        - time_based=1
        - runtime={{ workload_args.read_runtime }}
        - ramp_time={{ workload_args.read_ramp_time }}
    - jobname_match: rw
      params:
        - rwmixread=50
        - time_based=1
        - runtime={{ workload_args.read_runtime }}
        - ramp_time={{ workload_args.read_ramp_time }}
    - jobname_match: readwrite
      params:
        - rwmixread=50
        - time_based=1
        - runtime={{ workload_args.read_runtime }}
        - ramp_time={{ workload_args.read_ramp_time }}
    - jobname_match: randread
      params:
        - time_based=1
        - runtime={{ workload_args.read_runtime }}
        - ramp_time={{ workload_args.read_ramp_time }}
    - jobname_match: randwrite
      params:
        - time_based=1
        - runtime={{ workload_args.write_runtime }}
        - ramp_time={{ workload_args.write_ramp_time }}
    - jobname_match: randrw
      params:
        - time_based=1
        - runtime={{ workload_args.write_runtime }}
        - ramp_time={{ workload_args.write_ramp_time }}
#    - jobname_match: <search_string>
#      params:
#        - key=value
